{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a129f57-ed6f-464a-bb35-11b01c7c9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets torch scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc6b5e6-17ea-4ea0-b7e7-dff689b5dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f672ac7-4715-4174-96f7-870b0fca3287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a3cd3d-fb4d-4901-8bd2-cdabaefa1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(500))\n",
    "test_dataset  = dataset[\"test\"].select(range(5))  # tiny for quick runs per assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1538d67a-150c-415a-874f-ae2ed130c515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d078c4ef82bf4eab8b107c7221c88d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "MAX_LEN = 128  # you can tune this later\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # amazon_polarity has fields: \"title\", \"content\", \"label\"\n",
    "    # We'll combine title + content for stronger signal.\n",
    "    text = (example[\"title\"] + \" \" + example[\"content\"]).strip()\n",
    "\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function)\n",
    "test_dataset  = test_dataset.map(tokenize_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a7dee9-bbed-4c49-8770-ad731552963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a52c19c-ee88-4c55-a226-8d27b581fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # tune later\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95386795-b6c4-44a0-ae46-ca52f64707a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", num_labels=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output  # [batch, hidden]\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cca610d-d435-4eab-828e-51ca6a9a5820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60919c56276b4c978f58329975605798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "model = CustomBERTClassifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simple Adam optimizer (fine for assignment). You can switch to AdamW if you want.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c67f1f-9bd1-44d3-ac60-927889e4b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    avg_acc = correct / max(total, 1)\n",
    "    print(f\"Train Loss: {avg_loss:.4f} | Train Acc: {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf5e4b3-878a-4586-9140-72ca3d778693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    avg_acc = correct / max(total, 1)\n",
    "\n",
    "    print(f\"Eval Loss: {avg_loss:.4f} | Eval Acc: {avg_acc:.4f}\")\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb93e540-ca75-4508-8411-2d0469a43941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:46<00:00,  8.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6345 | Train Acc: 0.6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.4368 | Eval Acc: 0.8000\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:51<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3220 | Train Acc: 0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.5501 | Eval Acc: 0.8000\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:44<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1987 | Train Acc: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3715 | Eval Acc: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    labels, preds = evaluate(model, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40e2070-3ab5-4d14-9e36-86c3f6134e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    1.0000    0.6667         1\n",
      "           1     1.0000    0.7500    0.8571         4\n",
      "\n",
      "    accuracy                         0.8000         5\n",
      "   macro avg     0.7500    0.8750    0.7619         5\n",
      "weighted avg     0.9000    0.8000    0.8190         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f8e9e2-45fa-4116-963a-6ceb57eb2a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    model.eval()\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    token_type_ids = encoded.get(\"token_type_ids\", None)\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return label_map[pred]\n",
    "\n",
    "sample_text = \"The product quality was amazing and it exceeded my expectations!\"\n",
    "print(\"Predicted sentiment:\", predict_sentiment(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8c78394-7acc-49ac-afe1-c3e51f657b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTClassifierV2(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", num_labels=2, dropout=0.3, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        x = outputs.pooler_output\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout2(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d4a72a-5d10-4ec0-a39a-70c01dac4dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c39ce2a8dc4d0997b2d0ba507ac052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "model_v2 = CustomBERTClassifierV2(hidden_dim=256, dropout=0.3).to(device)\n",
    "criterion_v2 = nn.CrossEntropyLoss()\n",
    "optimizer_v2 = torch.optim.Adam(model_v2.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f03b76-077a-4a67-b12d-a76df964e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_factory, train_ds, test_ds, *, batch_size, lr, epochs, name):\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    model = model_factory().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n[{name}] Epoch {epoch+1}/{epochs}\")\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        labels, preds = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = sum(int(p == y) for p, y in zip(preds, labels)) / max(len(labels), 1)\n",
    "\n",
    "    result = {\n",
    "        \"experiment\": name,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"report\": classification_report(labels, preds, digits=4)\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501582d0-36fe-42ff-afc1-e5ead7d6dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_factory():\n",
    "    return CustomBERTClassifier(dropout=0.3)\n",
    "\n",
    "def v2_factory():\n",
    "    return CustomBERTClassifierV2(hidden_dim=256, dropout=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c223378-3e8e-465b-9072-998a348660e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTClassifierV3(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", num_labels=2, dropout=0.3, h1=256, h2=128):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.bert.config.hidden_size, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(h2, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        x = outputs.pooler_output\n",
    "        return self.net(x)\n",
    "\n",
    "def v3_factory():\n",
    "    return CustomBERTClassifierV3(dropout=0.3, h1=256, h2=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503a8fde-529b-42af-84ee-4aa4f6830f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_metrics(model, dataloader, criterion, device):\n",
    "    labels, preds = evaluate(model, dataloader, criterion, device)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = sum(int(p == y) for p, y in zip(preds, labels)) / max(len(labels), 1)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, digits=4))\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26acd49e-4dfb-4823-912a-5085bec9af90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa37a451d30d444292a6ba4c86562f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5] Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 16/16 [04:30<00:00, 16.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6832 | Train Acc: 0.5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6309 | Eval Acc: 0.8000\n",
      "\n",
      "[Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5] Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 16/16 [04:15<00:00, 15.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5834 | Train Acc: 0.7660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.5788 | Eval Acc: 0.8000\n",
      "\n",
      "[Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5] Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 16/16 [04:21<00:00, 16.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3520 | Train Acc: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.5986 | Eval Acc: 0.8000\n",
      "\n",
      "[Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5] Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 16/16 [04:18<00:00, 16.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2224 | Train Acc: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.7918 | Eval Acc: 0.6000\n",
      "\n",
      "[Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5] Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 16/16 [04:08<00:00, 15.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1313 | Train Acc: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6157 | Eval Acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493ef20ab81747a099e978a3a994263d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5] Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:46<00:00,  8.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6902 | Train Acc: 0.5240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6611 | Eval Acc: 0.8000\n",
      "\n",
      "[Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5] Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:40<00:00,  8.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5048 | Train Acc: 0.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3454 | Eval Acc: 0.8000\n",
      "\n",
      "[Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5] Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [24:16<00:00, 45.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2463 | Train Acc: 0.9320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3403 | Eval Acc: 0.8000\n",
      "\n",
      "[Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5] Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:31<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1556 | Train Acc: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.1947 | Eval Acc: 1.0000\n",
      "\n",
      "[Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5] Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:39<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0904 | Train Acc: 0.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.1073 | Eval Acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2221ecd82ee4c1389566ff16942a6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5] Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [11:22<00:00, 21.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6794 | Train Acc: 0.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6859 | Eval Acc: 0.8000\n",
      "\n",
      "[LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5] Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:31<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6129 | Train Acc: 0.7340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.5133 | Eval Acc: 0.8000\n",
      "\n",
      "[LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5] Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [33:28<00:00, 62.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4282 | Train Acc: 0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3221 | Eval Acc: 0.8000\n",
      "\n",
      "[LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5] Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:44<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2606 | Train Acc: 0.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.1621 | Eval Acc: 1.0000\n",
      "\n",
      "[LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5] Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:42<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1587 | Train Acc: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3453 | Eval Acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93868a449f8453bbe5f51487c8a8651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5] Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:56<00:00,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6956 | Train Acc: 0.4980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6794 | Eval Acc: 0.8000\n",
      "\n",
      "[V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5] Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:43<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6929 | Train Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.7116 | Eval Acc: 0.2000\n",
      "\n",
      "[V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5] Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:44<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6424 | Train Acc: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6144 | Eval Acc: 0.6000\n",
      "\n",
      "[V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5] Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:44<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4343 | Train Acc: 0.8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.4784 | Eval Acc: 0.8000\n",
      "\n",
      "[V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5] Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:34<00:00,  8.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2742 | Train Acc: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.4045 | Eval Acc: 0.8000\n",
      "\n",
      "Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5\n",
      "Acc=0.8000 | P=1.0000 | R=0.7500 | F1=0.8571\n",
      "\n",
      "Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5\n",
      "Acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
      "\n",
      "LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5\n",
      "Acc=0.8000 | P=1.0000 | R=0.7500 | F1=0.8571\n",
      "\n",
      "V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5\n",
      "Acc=0.8000 | P=0.8000 | R=1.0000 | F1=0.8889\n",
      "\n",
      "BEST RUN (by F1): Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5\n",
      "Acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\shweiss\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\shweiss\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\shweiss\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Combo 1 (your original): V2 head, bs=32, lr=2e-5, epochs=5\n",
    "results.append(\n",
    "    run_experiment(\n",
    "        v2_factory, train_dataset, test_dataset,\n",
    "        batch_size=32, lr=2e-5, epochs=5,\n",
    "        name=\"Combo1: V2 head (2-layer), epochs=5, bs=32, lr=2e-5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combo 2 (your original): V2 head, bs=16, lr=2e-5, epochs=5\n",
    "results.append(\n",
    "    run_experiment(\n",
    "        v2_factory, train_dataset, test_dataset,\n",
    "        batch_size=16, lr=2e-5, epochs=5,\n",
    "        name=\"Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# REQUIRED: Learning rate experiment (same V2 head, different LR)\n",
    "results.append(\n",
    "    run_experiment(\n",
    "        v2_factory, train_dataset, test_dataset,\n",
    "        batch_size=16, lr=1e-5, epochs=5,\n",
    "        name=\"LR Test: V2 head (2-layer), epochs=5, bs=16, lr=1e-5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# REQUIRED: V3 experiment (deeper head)\n",
    "results.append(\n",
    "    run_experiment(\n",
    "        v3_factory, train_dataset, test_dataset,\n",
    "        batch_size=16, lr=2e-5, epochs=5,\n",
    "        name=\"V3 Test: deeper head (3-layer), epochs=5, bs=16, lr=2e-5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print summary for each experiment\n",
    "for r in results:\n",
    "    print(\"\\n\" + r[\"experiment\"])\n",
    "    print(f\"Acc={r['accuracy']:.4f} | P={r['precision']:.4f} | R={r['recall']:.4f} | F1={r['f1']:.4f}\")\n",
    "\n",
    "# Show best run by F1 \n",
    "best = max(results, key=lambda x: x[\"f1\"])\n",
    "print(\"\\nBEST RUN (by F1):\", best[\"experiment\"])\n",
    "print(f\"Acc={best['accuracy']:.4f} | P={best['precision']:.4f} | R={best['recall']:.4f} | F1={best['f1']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c944ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          experiment  batch_size       lr  \\\n",
      "1  Combo2: V2 head (2-layer), epochs=5, bs=16, lr...          16  0.00002   \n",
      "3  V3 Test: deeper head (3-layer), epochs=5, bs=1...          16  0.00002   \n",
      "0  Combo1: V2 head (2-layer), epochs=5, bs=32, lr...          32  0.00002   \n",
      "2  LR Test: V2 head (2-layer), epochs=5, bs=16, l...          16  0.00001   \n",
      "\n",
      "   epochs  accuracy  precision  recall        f1  \n",
      "1       5       1.0        1.0    1.00  1.000000  \n",
      "3       5       0.8        0.8    1.00  0.888889  \n",
      "0       5       0.8        1.0    0.75  0.857143  \n",
      "2       5       0.8        1.0    0.75  0.857143  \n",
      "\n",
      "BEST RUN: Combo2: V2 head (2-layer), epochs=5, bs=16, lr=2e-5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if \"results\" not in globals() or not results:\n",
    "    raise RuntimeError(\"Run the experiments cell first so `results` exists.\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "cols = [\"experiment\", \"batch_size\", \"lr\", \"epochs\", \"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "df_results[cols].to_csv(\"bert_experiment_results.csv\", index=False)\n",
    "\n",
    "best = max(results, key=lambda x: x[\"f1\"])\n",
    "with open(\"best_run_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(best[\"experiment\"] + \"\\n\\n\" + best[\"report\"])\n",
    "\n",
    "print(df_results[cols].sort_values(\"f1\", ascending=False))\n",
    "print(\"\\nBEST RUN:\", best[\"experiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cbf28ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860122308e7441c0bcd365a657df0144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:57<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6824 | Train Acc: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:47<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4647 | Train Acc: 0.8780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:29<00:00,  8.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2119 | Train Acc: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:40<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1100 | Train Acc: 0.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 32/32 [04:42<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0552 | Train Acc: 0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.0219 | Eval Acc: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1 Score:  1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         1\n",
      "           1     1.0000    1.0000    1.0000         4\n",
      "\n",
      "    accuracy                         1.0000         5\n",
      "   macro avg     1.0000    1.0000    1.0000         5\n",
      "weighted avg     1.0000    1.0000    1.0000         5\n",
      "\n",
      "Saved: best_bert_model_v2.pth\n",
      "Saved: best_bert_model_v2_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if \"results\" not in globals() or not results:\n",
    "    raise RuntimeError(\"Run the experiments cell first so `results` exists.\")\n",
    "\n",
    "best = max(results, key=lambda x: x[\"f1\"])\n",
    "\n",
    "batch_size = int(best[\"batch_size\"])\n",
    "lr = float(best[\"lr\"])\n",
    "epochs = int(best[\"epochs\"])\n",
    "\n",
    "exp_name = best[\"experiment\"].lower()\n",
    "if \"v3\" in exp_name or \"deeper\" in exp_name or \"3-layer\" in exp_name:\n",
    "    model_factory = v3_factory\n",
    "    model_tag = \"v3\"\n",
    "else:\n",
    "    model_factory = v2_factory\n",
    "    model_tag = \"v2\"\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = model_factory().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    if \"train_one_epoch\" in globals():\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    else:\n",
    "        train(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "_ = evaluate_with_metrics(model, test_loader, criterion, device)\n",
    "\n",
    "weights_path = f\"best_bert_model_{model_tag}.pth\"\n",
    "torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "config_path = f\"best_bert_model_{model_tag}_config.json\"\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_experiment\": best[\"experiment\"],\n",
    "            \"model_tag\": model_tag,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": epochs,\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "print(\"Saved:\", weights_path)\n",
    "print(\"Saved:\", config_path)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
